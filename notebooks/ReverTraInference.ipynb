{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **ReverTra - Reverse Translation with Transformers**\n",
        "\n",
        "---\n",
        "\n",
        "\\\n",
        "This is an inference script for using the models from the paper [ref]. \\\n",
        "The models are built on the BART architecture from the Huggingface platform, and the training procedure and algorithms are depicted in the paper. \\\n",
        "Github code with full training code and data can be found: [here]. \\\n",
        "\n",
        "This script accomodates the two inference types with BART models depicted in the paper: (a) only the target amino-acid sequence; and (b) alignment of the target amino-acid sequence with an additional codon sequence.\n",
        "\n",
        "This script also offers an option to include accuracy and perplexity calculation given the original codon target sequence is present.  calculating accuracy and loss for a specific sequence generation, we offer an option to include the targe codon sequence and get the statistics.\n",
        "\n",
        "**Available models:**\n",
        "\n",
        "**[Mimic]**\n",
        "1. Finetuned twice mimic model with fixed-win of size 10: \"siditom/co-model_mimic-rexpr-10w_2ft\" \\\n",
        "2. Finetuned twice mimic model with fixed-win of size 30: \"siditom/co-model_mimic-rexpr-30w_2ft\" \\\n",
        "3. Finetuned twice mimic model with fixed-win of size 50: \"siditom/co-model_mimic-rexpr-50w_2ft\" \\\n",
        "4. Finetuned twice mimic model with fixed-win of size 75: \"siditom/co-model_mimic-rexpr-75w_2ft\" \\\n",
        "5. Finetuned twice mimic model with fixed-win of size 100: \"siditom/co-model_mimic-rexpr-100w_2ft\" \\\n",
        "6. Finetuned twice mimic model with fixed-win of size 150: \"siditom/co-model_mimic-rexpr-150w_2ft\" \\\n",
        "\n",
        "**[Mask]** \\\n",
        "1. Finetuned once mask model with fixed-win of size 10: \"siditom/co-model_mask-rexpr-10w_1ft\" \\\n",
        "2. Finetuned once mask model with fixed-win of size 30: \"siditom/co-model_mask-rexpr-30w_1ft\" \\\n",
        "3. Finetuned once mask model with fixed-win of size 50: \"siditom/co-model_mask-rexpr-50w_1ft\" \\\n",
        "4. Finetuned once mask model with fixed-win of size 75: \"siditom/co-model_mask-rexpr-75w_1ft\" \\\n",
        "5. Finetuned once mask model with fixed-win of size 100: \"siditom/co-model_mask-rexpr-100w_1ft\" \\\n",
        "6. Finetuned once mask model with fixed-win of size 150: \"siditom/co-model_mask-rexpr-150w_1ft\" \\"
      ],
      "metadata": {
        "id": "YRBUNDB-pETK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementation details:\n",
        "---\n",
        "\n",
        "The arguemnts for the predict function below includes a dictionary configuration file with the paramenters needed for the inference:\n",
        "\n",
        "- **sw_aa_size**: window size for generating subsets of predictions - should be the same size as the model was trained on. Options: [10,30,50,75,100,150], see available models.\n",
        "- **inference_type**: 'mimic'/'mask'.\n",
        "- **calc_stats**: False/True. Whether the input includes the target codon sequence for calculating accuracy and perplexity.\n",
        "\n",
        "The input args are also a dictionary with the following keys:\n",
        "- **qseq**: amino-acid sequence of the target sequence.\n",
        "- **query_species**: the traget's host species.\n",
        "- **expr**: the token of the expression level. we enable 6 tokens corresponding for the expression level percentails: 90%-100%, 75%-90%, 50%-75%, 25%-50%, lower than 25%, and unspecified with tokens: [expr_top10, expr_pre75_90, expr_pre50_75, expr_pre25_50, expr_low25, expr_unk], respectively.\n",
        "- **subject_dna_seq**: [Optional] - space delimited codon sequence of the mimic protein. Required for inference_type='mimic'.\n",
        "- **query_dna_seq**: [Optional] - space delimited codon sequence of the target protein. Required for calc_stats=True.\n",
        "- **subject_species**:  [Optional] - the mimic's sequence origin species. Required for inference_type='mimic'.\n",
        "\n"
      ],
      "metadata": {
        "id": "vzB1nDz9Jdl6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUQlJd5i_D_B",
        "outputId": "4a0ff0ab-7480-42bc-9c14-3cecd3e46800"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "JRnVB4Zg_hqu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2152f75-c467-45d1-d297-fe4e967caf12"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, BartForConditionalGeneration, LogitsWarper\n",
        "from transformers import LogitsProcessor,LogitsProcessorList\n",
        "import numpy as np\n",
        "import json"
      ],
      "metadata": {
        "id": "LJEp-XlizbA0"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RestrictToAaLogitsWarper(LogitsWarper):\n",
        "    def __init__(self, masked_input_ids: torch.LongTensor, restrict_dict: dict, filter_value: float = -float(\"Inf\"), min_tokens_to_keep: int = 1):\n",
        "        self.masked_input_ids = masked_input_ids\n",
        "        self.restrict_dict = restrict_dict\n",
        "        self.filter_value = filter_value\n",
        "        self.min_tokens_to_keep = min_tokens_to_keep\n",
        "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
        "        cur_len = input_ids.shape[-1]-1\n",
        "        vocab_size = scores.shape[-1]\n",
        "        if self.masked_input_ids.shape[-1] <= cur_len:\n",
        "            return scores\n",
        "        for bid in range(input_ids.shape[0]):\n",
        "            cur_mask_input = str(int(self.masked_input_ids[bid][cur_len].item()))\n",
        "            if cur_mask_input in self.restrict_dict.keys():\n",
        "                restricted_words = self.restrict_dict[cur_mask_input]\n",
        "                banned_indices = set(range(vocab_size))-set(self.restrict_dict[cur_mask_input])\n",
        "                banned = torch.tensor([i in banned_indices for i in range(vocab_size)])\n",
        "                scores[bid][banned] = self.filter_value\n",
        "        return scores\n",
        "\n"
      ],
      "metadata": {
        "id": "vnOTaX8B2Y1o"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_expr_token(expr):\n",
        "    if expr==None:\n",
        "        expr = 'expr_unk'\n",
        "    return \"<\"+expr+\"> \"\n",
        "\n",
        "def prepare_inputs(example, tokenizer, config):\n",
        "        #Rename\n",
        "        qaaseq, query_species, sw_aa_size = example['qseq'], example['query_species'],  config['sw_aa_size']\n",
        "        if config['inference_type']=='mimic':\n",
        "          sseq, subject_species = example['subject_dna_seq'].split(\" \"), example['subject_species']\n",
        "        else:\n",
        "          sseq, subject_species = ['<mask_'+aa+'>' if aa!='-' else '<gap>' for aa in qaaseq], example['query_species']\n",
        "\n",
        "\n",
        "        #Prepare fixed-sized windows\n",
        "        query_aa_wins = [qaaseq[i:i+sw_aa_size] for i in range(0,max(1,len(qaaseq)-sw_aa_size+1))]\n",
        "        subject_dna_wins = [\" \".join(sseq[i:i+sw_aa_size]) for i in range(0,max(1,len(qaaseq)-sw_aa_size+1))]\n",
        "        mask_aa_wins = [\"<\"+query_species+\"> \"+\" \".join(['<mask_'+aa+'>' if aa!='-' else '<gap>' for aa in wseq]) for wseq in query_aa_wins]\n",
        "        query_aa_wins = [\"<\"+query_species+\"> \"+get_expr_token(example['expr'])+' '.join(['<mask_'+aa+'>' if aa!='-' else '<gap>' for aa in wseq]) for wseq in query_aa_wins]\n",
        "        subject_dna_wins = [\"<\"+subject_species+\"> \"+wseq for wseq in subject_dna_wins]\n",
        "\n",
        "        #Encode windows\n",
        "        input_ids = tokenizer(query_aa_wins, subject_dna_wins, return_tensors=\"pt\", padding='max_length', max_length=sw_aa_size*2+3).input_ids\n",
        "        masked_ids = tokenizer(mask_aa_wins, return_tensors=\"pt\").input_ids[:,1:-1]\n",
        "        return input_ids,masked_ids\n",
        "\n",
        "def generate_outputs(input_ids, masked_ids, mask_restriction_dict, model, sw_aa_size):\n",
        "        logits_processor = LogitsProcessorList(\n",
        "                [RestrictToAaLogitsWarper(masked_ids, mask_restriction_dict)])\n",
        "\n",
        "        outputs = model.generate(input_ids, do_sample=False, output_scores = True, return_dict_in_generate = True, renormalize_logits = True, logits_processor=logits_processor, max_length=min((sw_aa_size+3),masked_ids.shape[-1]+2))\n",
        "        outputs = torch.stack(outputs['scores'][:sw_aa_size+1],1)\n",
        "        return outputs\n",
        "\n",
        "def calc_combined_gen_from_sliding_windows_logits(sw_logits, seqlen, sw_aa_size):\n",
        "        sw_logits = sw_logits\n",
        "        collect_logits = torch.zeros([seqlen, sw_logits.shape[-1]])\n",
        "        counts = torch.zeros([1,seqlen])\n",
        "        most_freq_pred = torch.zeros([seqlen,1])\n",
        "\n",
        "        #Aggregating (sums) the logits of the different windows. Only the relevant codons (restricted by AA) are sumed.\n",
        "        for i in range(sw_logits.shape[0]): # window num\n",
        "            for j in range(min(sw_aa_size, seqlen)): # sequence len - codon index\n",
        "                collect_logits[i+j, :] += torch.exp(sw_logits[i, 1+j, :])\n",
        "                counts[0,i+j] += 1\n",
        "\n",
        "        #Normalizing each position by the number of predictions (eg. first codon has only one prediction)\n",
        "        for i in range(seqlen):\n",
        "            collect_logits[i,:] /= counts[0,i]\n",
        "        collect_logits = torch.log(collect_logits)\n",
        "        collect_logits = collect_logits.log_softmax(dim=-1)\n",
        "\n",
        "        for i in range(seqlen):\n",
        "            most_freq_pred[i] = torch.argmax(collect_logits[i,:]).item()\n",
        "\n",
        "        return collect_logits, most_freq_pred\n",
        "\n",
        "def predict(config, example, mask_restriction_dict, tokenizer, model):\n",
        "\n",
        "\n",
        "\n",
        "        input_ids, masked_ids = prepare_inputs(example, tokenizer, config)\n",
        "        outputs = generate_outputs(input_ids, masked_ids, mask_restriction_dict, model, config['sw_aa_size'])\n",
        "        logits, most_freq_pred = calc_combined_gen_from_sliding_windows_logits(outputs, len(example['qseq']), config['sw_aa_size'])\n",
        "\n",
        "        ce = torch.nn.CrossEntropyLoss()\n",
        "        most_freq_pred=most_freq_pred.clone().detach().reshape((1,-1))\n",
        "\n",
        "\n",
        "        #print(\"decode: \", tokenizer.decode(most_freq_pred.numpy().astype(int)[0]))\n",
        "        #print(\"truevals: \", tokenizer.decode(true_vals))\n",
        "        res = dict()\n",
        "\n",
        "        res['prot_len'] = len(example['qseq'])\n",
        "        res['prot_AAs'] = example['qseq']\n",
        "        res['pred_codons'] = tokenizer.decode(most_freq_pred.numpy().astype(int)[0])\n",
        "        res['entropy'] = (-torch.nan_to_num(torch.exp(logits)*logits,nan=0.0).sum(dim=-1)).mean().item()\n",
        "\n",
        "        assert(res['prot_len']==len(res['pred_codons'].split(\" \")))\n",
        "\n",
        "        if config['calc_stats'] and 'query_dna_seq' in example.keys():\n",
        "          true_vals = tokenizer(example['query_dna_seq'], return_tensors=\"pt\").input_ids[:,1:-1]\n",
        "          mask = true_vals > 41 #special tokens threshold\n",
        "          true_vals = true_vals.tolist()[0]\n",
        "          masked_most_freq_pred = most_freq_pred.masked_select(mask).numpy().astype(int)\n",
        "          masked_true_vals = torch.tensor(true_vals).masked_select(mask).numpy().astype(int)\n",
        "\n",
        "          res['subject_codons'] = example['subject_dna_seq']\n",
        "          res['num_of_correct_predicted_codons'] = sum([int(x==y) for x,y in zip(masked_true_vals, masked_most_freq_pred)])\n",
        "          res['query_codons'] = example['query_dna_seq']\n",
        "          res['cross_entropy_loss'] = ce(logits, torch.tensor(true_vals)).item()\n",
        "          res['perplexity'] = np.exp(res['cross_entropy_loss'])\n",
        "          res['accuracy'] = res['num_of_correct_predicted_codons'] / res['prot_len']\n",
        "        #print(example['qseqid'], example['sseqid'],res['cross_entropy_loss'], res['entropy'],res['accuracy'])\n",
        "        return res"
      ],
      "metadata": {
        "id": "UaHH3ZKly7BD"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('siditom/tokenizer-codon_optimization-refined_expr')\n",
        "!wget https://huggingface.co/siditom/tokenizer-codon_optimization-refined_expr/resolve/main/mask_restrict_dict.json\n",
        "mask_restrict_dict = {}\n",
        "with open('/content/mask_restrict_dict.json','r') as handle:\n",
        "  mask_restrict_dict = json.load(handle)\n",
        "model = BartForConditionalGeneration.from_pretrained(\"siditom/co-model_mimic-rexpr-50w_2ft\")\n"
      ],
      "metadata": {
        "id": "GcSdzMdvyXaz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1d3e16a-31f9-48bd-c4a1-4865103e5401"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-05 13:06:22--  https://huggingface.co/siditom/tokenizer-codon_optimization-refined_expr/resolve/main/mask_restrict_dict.json\n",
            "Resolving huggingface.co (huggingface.co)... 65.8.178.93, 65.8.178.27, 65.8.178.118, ...\n",
            "Connecting to huggingface.co (huggingface.co)|65.8.178.93|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1457 (1.4K) [text/plain]\n",
            "Saving to: ‘mask_restrict_dict.json.7’\n",
            "\n",
            "\r          mask_rest   0%[                    ]       0  --.-KB/s               \rmask_restrict_dict. 100%[===================>]   1.42K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-11-05 13:06:22 (635 MB/s) - ‘mask_restrict_dict.json.7’ saved [1457/1457]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Example 1 - mimic codon sequence generation**"
      ],
      "metadata": {
        "id": "3oWUZPVFjjd3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "config = {\n",
        "    'sw_aa_size':50,\n",
        "    'calc_stats':False,\n",
        "    'inference_type':'mimic'\n",
        "}\n",
        "\n",
        "example = {\n",
        "    'subject_dna_seq':'AAA GCG GCT GTA CTG GTC AAG AAA GTT CTG GAA TCT GCC ATT GCT AAC GCT GAA CAC AAC GAT GGC GCT GAC ATT GAC GAT CTG AAA GTT ACG AAA ATT TTC GTA GAC GAA GGC CCG AGC ATG AAG CGC ATT ATG CCG CGT GCA AAA GGT CGT GCA GAT CGC ATC CTG AAG CGC ACC AGC CAC ATC ACT GTG GTT GTG TCC GAT CGC',\n",
        "    'qseq':'KSVKFVQGLLQNAAANAEA-KGLDATKLYVSHIQVNQAPKQRRRTYRAHGRINKYESSPSHIELVVTEK',\n",
        "    'query_species':'S_cerevisiae',\n",
        "    'subject_species':'E_coli',\n",
        "    'expr':'expr_top10'\n",
        "}\n",
        "\n",
        "assert(len(example['qseq'])==len(example['subject_dna_seq'].split(\" \")))\n",
        "res = json.dumps(predict(config, example, mask_restrict_dict, tokenizer, model),indent=2)\n",
        "print(res)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJdsHphO1OUE",
        "outputId": "de2871ff-4258-4405-86e2-1d1435a5cf84"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"prot_len\": 69,\n",
            "  \"prot_AAs\": \"KSVKFVQGLLQNAAANAEA-KGLDATKLYVSHIQVNQAPKQRRRTYRAHGRINKYESSPSHIELVVTEK\",\n",
            "  \"pred_codons\": \"AAG TCT GTT AAG TTT GTT CAA GGT TTG TTG CAA AAC GCT GCT GCT AAC GCT GAA GCT <gap> AAG GGT TTG GAT GCT ACC AAG TTG TAC GTT TCT CAC ATT CAA GTC AAC CAA GCT CCA AAG CAA AGA AGA AGA ACT TAC AGA GCT CAC GGT AGA ATC AAC AAG TAC GAA TCT TCT CCA TCT CAC ATT GAA TTG GTT GTT ACT GAA AAG\",\n",
            "  \"entropy\": 0.7021335959434509\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Example 2 - mimic codon sequence generation with statistics**\n",
        "\n",
        "This inference enables you to calculate the accuracy and loss for a specific codon sequence. In addition with the amino-acid sequence of the protein target, you are required to insert the codon sequence of the translated protein."
      ],
      "metadata": {
        "id": "EnEYcp5Fj8ux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    'sw_aa_size':50,\n",
        "    'calc_stats':True,\n",
        "    'inference_type':'mimic'\n",
        "}\n",
        "\n",
        "example = {\n",
        "    'query_dna_seq':  'AAA TCT GTT AAG TTC GTT CAA GGT TTG TTG CAA AAC GCC GCT GCC AAT GCT GAA GCT <gap> AAG GGT CTA GAT GCT ACC AAG TTG TAC GTT TCT CAC ATC CAA GTT AAC CAA GCA CCA AAG CAA AGA AGA AGA ACT TAC AGA GCC CAC GGT AGA ATC AAC AAG TAC GAA TCT TCT CCA TCT CAC ATT GAA TTG GTT GTT ACC GAA AAG',\n",
        "    'subject_dna_seq':'AAA GCG GCT GTA CTG GTC AAG AAA GTT CTG GAA TCT GCC ATT GCT AAC GCT GAA CAC AAC GAT GGC GCT GAC ATT GAC GAT CTG AAA GTT ACG AAA ATT TTC GTA GAC GAA GGC CCG AGC ATG AAG CGC ATT ATG CCG CGT GCA AAA GGT CGT GCA GAT CGC ATC CTG AAG CGC ACC AGC CAC ATC ACT GTG GTT GTG TCC GAT CGC',\n",
        "    'qseq':'KSVKFVQGLLQNAAANAEA-KGLDATKLYVSHIQVNQAPKQRRRTYRAHGRINKYESSPSHIELVVTEK',\n",
        "    'query_species':'S_cerevisiae',\n",
        "    'subject_species':'E_coli',\n",
        "    'expr':'expr_top10'\n",
        "}\n",
        "\n",
        "assert(len(example['qseq'])==len(example['query_dna_seq'].split(\" \")))\n",
        "assert(len(example['qseq'])==len(example['subject_dna_seq'].split(\" \")))\n",
        "res = json.dumps(predict(config, example, mask_restrict_dict, tokenizer, model),indent=2)\n",
        "print(res)"
      ],
      "metadata": {
        "id": "2KsYj_gaJ8ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3bef411-c9be-486a-ebc3-431393edf9ca"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"prot_len\": 69,\n",
            "  \"prot_AAs\": \"KSVKFVQGLLQNAAANAEA-KGLDATKLYVSHIQVNQAPKQRRRTYRAHGRINKYESSPSHIELVVTEK\",\n",
            "  \"pred_codons\": \"AAG TCT GTT AAG TTT GTT CAA GGT TTG TTG CAA AAC GCT GCT GCT AAC GCT GAA GCT <gap> AAG GGT TTG GAT GCT ACC AAG TTG TAC GTT TCT CAC ATT CAA GTC AAC CAA GCT CCA AAG CAA AGA AGA AGA ACT TAC AGA GCT CAC GGT AGA ATC AAC AAG TAC GAA TCT TCT CCA TCT CAC ATT GAA TTG GTT GTT ACT GAA AAG\",\n",
            "  \"entropy\": 0.7021335959434509,\n",
            "  \"subject_codons\": \"AAA GCG GCT GTA CTG GTC AAG AAA GTT CTG GAA TCT GCC ATT GCT AAC GCT GAA CAC AAC GAT GGC GCT GAC ATT GAC GAT CTG AAA GTT ACG AAA ATT TTC GTA GAC GAA GGC CCG AGC ATG AAG CGC ATT ATG CCG CGT GCA AAA GGT CGT GCA GAT CGC ATC CTG AAG CGC ACC AGC CAC ATC ACT GTG GTT GTG TCC GAT CGC\",\n",
            "  \"num_of_correct_predicted_codons\": 57,\n",
            "  \"query_codons\": \"AAA TCT GTT AAG TTC GTT CAA GGT TTG TTG CAA AAC GCC GCT GCC AAT GCT GAA GCT <gap> AAG GGT CTA GAT GCT ACC AAG TTG TAC GTT TCT CAC ATC CAA GTT AAC CAA GCA CCA AAG CAA AGA AGA AGA ACT TAC AGA GCC CAC GGT AGA ATC AAC AAG TAC GAA TCT TCT CCA TCT CAC ATT GAA TTG GTT GTT ACC GAA AAG\",\n",
            "  \"cross_entropy_loss\": 0.5106220245361328,\n",
            "  \"perplexity\": 1.6663273691584313,\n",
            "  \"accuracy\": 0.8260869565217391\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Example 3 - mask codon sequence generation**\n",
        "\n",
        "This inference enables you to calculate the accuracy and loss for a specific codon sequence. In addition with the amino-acid sequence of the protein target, you are required to insert the codon sequence of the translated protein."
      ],
      "metadata": {
        "id": "7GCITywyldg0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    'sw_aa_size':50,\n",
        "    'calc_stats':False,\n",
        "    'inference_type':'mask'\n",
        "\n",
        "}\n",
        "\n",
        "example = {\n",
        "    'qseq':'KSVKFVQGLLQNAAANAEA-KGLDATKLYVSHIQVNQAPKQRRRTYRAHGRINKYESSPSHIELVVTEK',\n",
        "    'query_species':'S_cerevisiae',\n",
        "    'expr':'expr_top10'\n",
        "}\n",
        "\n",
        "res = json.dumps(predict(config, example, mask_restrict_dict, tokenizer, model),indent=2)\n",
        "print(res)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtKiKtfbQ6eE",
        "outputId": "2dfcd1a8-0545-47e3-fadb-dcb7c5b56970"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"prot_len\": 69,\n",
            "  \"prot_AAs\": \"KSVKFVQGLLQNAAANAEA-KGLDATKLYVSHIQVNQAPKQRRRTYRAHGRINKYESSPSHIELVVTEK\",\n",
            "  \"pred_codons\": \"AAG TCT GTT AAG TTC GTT CAA GGT TTG TTG CAA AAC GCT GCT GCT AAC GCT GAA GCT <gap> AAG GGT TTG GAT GCT ACC AAG TTG TAC GTT TCT CAC ATT CAA GTC AAC CAA GCT CCA AAG CAA AGA AGA AGA ACT TAC AGA GCT CAC GGT AGA ATC AAC AAG TAC GAA TCT TCT CCA TCT CAC ATT GAA TTG GTT GTT ACT GAA AAG\",\n",
            "  \"entropy\": 0.66972815990448\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oNAvGJy-Q7U2"
      },
      "execution_count": 63,
      "outputs": []
    }
  ]
}